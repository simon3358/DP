{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import gensim.models as gsm\n",
    "import phrase2vec as p2v\n",
    "from utils import create_tweet_vectors, create_emoji_tweets, create_emoji_sentiment\n",
    "\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "from sklearn import metrics\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from scipy import stats\n",
    "\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Nacitanie dat a embeddingov\n",
    "- E2V data\n",
    "- emocontext data\n",
    "- anasent data\n",
    "- NLP modely"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. E2V dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tweets = pd.read_csv('D:\\Downloads\\DP\\data\\e2v_data.csv')\n",
    "tweets = pd.read_csv('D:\\Downloads\\DP\\data\\e2v_data_emoji.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Emocontext dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tweets = pd.read_csv('D:\\Downloads\\DP\\data\\emocontext.csv')\n",
    "tweets = pd.read_csv('D:\\Downloads\\DP\\data\\emocontext_emoji.csv')\n",
    "\n",
    "# tweets_train = pd.read_csv('D:/Downloads/DP/data/emocontext.csv')\n",
    "# tweets_dev = pd.read_csv('D:/Downloads/DP/data/emocontext_dev.csv')\n",
    "# tweets_test = pd.read_csv('D:/Downloads/DP/data/emocontext_test.csv')\n",
    "\n",
    "# tweets = pd.concat([tweets_train,tweets_dev,tweets_test])\n",
    "# tweets = tweets.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Anasent dataset (SK)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tweets = pd.read_csv('D:/Downloads/DP/data/anasent.csv')\n",
    "tweets = pd.read_csv('D:/Downloads/DP/data/anasent_emoji.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "84321"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tweets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# vektory pre anglictinu\n",
    "\n",
    "phrase_model_en = p2v.Phrase2Vec(\n",
    "    300,\n",
    "    gsm.KeyedVectors.load_word2vec_format('D:\\Downloads\\DP\\word2vec300_en.vec', binary=False),\n",
    "    gsm.KeyedVectors.load_word2vec_format('D:\\Downloads\\DP\\emoji2vec300.bin', binary=True)\n",
    ")\n",
    "\n",
    "phrase_model_no_e2v_en = p2v.Phrase2Vec(\n",
    "    300,\n",
    "    gsm.KeyedVectors.load_word2vec_format('D:\\Downloads\\DP\\word2vec300_en.vec', binary=False)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "18512 | INFO | loading projection weights from D:\\Downloads\\DP\\word2vec300_sk.vec\n",
      "18512 | INFO | loaded (2000000, 300) matrix from D:\\Downloads\\DP\\word2vec300_sk.vec\n",
      "18512 | INFO | loading projection weights from D:\\Downloads\\DP\\emoji2vec300.bin\n",
      "18512 | INFO | loaded (1661, 300) matrix from D:\\Downloads\\DP\\emoji2vec300.bin\n",
      "18512 | INFO | loading projection weights from D:\\Downloads\\DP\\word2vec300_sk.vec\n",
      "18512 | INFO | loaded (2000000, 300) matrix from D:\\Downloads\\DP\\word2vec300_sk.vec\n"
     ]
    }
   ],
   "source": [
    "# vektory pre slovencinu\n",
    "\n",
    "phrase_model_sk = p2v.Phrase2Vec(\n",
    "    300,\n",
    "    gsm.KeyedVectors.load_word2vec_format('D:\\Downloads\\DP\\word2vec300_sk.vec', binary=False),\n",
    "    gsm.KeyedVectors.load_word2vec_format('D:\\Downloads\\DP\\emoji2vec300.bin', binary=True)\n",
    ")\n",
    "\n",
    "phrase_model_no_e2v_sk = p2v.Phrase2Vec(\n",
    "    300,\n",
    "    gsm.KeyedVectors.load_word2vec_format('D:\\Downloads\\DP\\word2vec300_sk.vec', binary=False)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# vytvorenie subdatasetu s emotikonmi\n",
    "\n",
    "# create_emoji_tweets(tweets, phrase_model_en.emojiVecModel, 'D:\\Downloads\\DP\\data\\e2v_data_emoji.csv')\n",
    "# create_emoji_tweets(tweets, phrase_model_en.emojiVecModel, 'D:\\Downloads\\DP\\data\\emocontext_emoji.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Priprava dat\n",
    "- rozdelenie dat v zadanom pomere na podmnoziny, s rozdelenim labelov (predikovana hodnota)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pri EMOCONTEXT su pouzite niektore nove emotikony -> nahradime\n",
    "emo_mapping = {\n",
    "    'ðŸ™':'ðŸ˜•',\n",
    "    'ðŸ¤£':'ðŸ˜‚',\n",
    "    'ðŸ¤':'ðŸ˜¬',\n",
    "    'ðŸ™„':'ðŸ˜',\n",
    "    'ðŸ¾':'ðŸ¹',\n",
    "    'ðŸ¤—':'â˜º',\n",
    "    'ðŸ¤”':'ðŸ˜',\n",
    "    'ðŸ¤¡':'ðŸƒ',\n",
    "    'ðŸ¤‘':'ðŸ’°',\n",
    "    '\\u200d':'',\n",
    "    'ðŸ¤¥':'ðŸ˜¢',\n",
    "    'ðŸ¤·':'ðŸ’',\n",
    "}\n",
    "\n",
    "for i, j in emo_mapping.items():\n",
    "    tweets['Text'] = tweets['Text'].str.replace(i, j)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets = create_emoji_sentiment(tweets, phrase_model_sk.emojiVecModel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# anglictina\n",
    "\n",
    "tweets = tweets.sample(frac=1).reset_index(drop=True)\n",
    "\n",
    "train_x, train_y, valid_x, valid_y, test_x, test_y = create_tweet_vectors(tweets, phrase_model_no_e2v_en, 0.7, False)\n",
    "train_x_e2v, _, valid_x_e2v, _, test_x_e2v, _ = create_tweet_vectors(tweets, phrase_model_en, 0.7, False)\n",
    "\n",
    "train_x_sen, _, valid_x_sen, _, test_x_sen, _ = create_tweet_vectors(tweets, phrase_model_no_e2v_en, 0.7, True)\n",
    "train_x_e2v_sen, train_y, valid_x_e2v_sen, valid_y, test_x_e2v_sen, test_y = create_tweet_vectors(tweets, phrase_model_en, 0.7, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# slovencina\n",
    "\n",
    "tweets = tweets.sample(frac=1).reset_index(drop=True)\n",
    "\n",
    "train_x, train_y, valid_x, valid_y, test_x, test_y = create_tweet_vectors(tweets, phrase_model_no_e2v_sk, 0.7, False)\n",
    "train_x_e2v, _, valid_x_e2v, _, test_x_e2v, _ = create_tweet_vectors(tweets, phrase_model_sk, 0.7, False)\n",
    "\n",
    "train_x_sen, _, valid_x_sen, _, test_x_sen, _ = create_tweet_vectors(tweets, phrase_model_no_e2v_sk, 0.7, True)\n",
    "train_x_e2v_sen, _, valid_x_e2v_sen, _, test_x_e2v_sen, _ = create_tweet_vectors(tweets, phrase_model_sk, 0.7, True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vader - ziskanie baseline-u\n",
    "- vhodne len pre e2v dataset, kvoli rozdeleniu tried"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "vader = SentimentIntensityAnalyzer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets['Lex'] = tweets['Text'].apply(lambda x: vader.polarity_scores(x)['compound'])\n",
    "tweets['Lex_label'] = tweets['Lex'].apply(lambda x: 'Positive' if x > 0.05 else ('Neutral' if x > -0.05 else 'Negative')) \n",
    "# hranice 0.05 su dane od autora"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.4498642093903061"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metrics.accuracy_score(tweets['Label'], tweets['Lex_label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.4498642093903061"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metrics.f1_score(tweets['Label'], tweets['Lex_label'], average='micro')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predikcie (NB, RF, GBT)\n",
    "- overenie uspesnosti modelov:\n",
    "    - model (w2v)\n",
    "    - model (w2v, e2v) \n",
    "    - model (w2v, senti)\n",
    "    - model (w2v, e2v, senti)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def experiment_classic(model, train_x, train_y, test_x, test_y, text):\n",
    "    model.fit(train_x, train_y)\n",
    "    predict = model.predict(test_x)\n",
    "    acc = metrics.accuracy_score(test_y, predict)\n",
    "    f1 = metrics.f1_score(test_y, predict, average='micro')\n",
    "    print(text, \":   acc: \", acc,\" F1: \", f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def experiment(model, params, train_x, train_y, valid_x, valid_y, test_x, test_y, text):\n",
    "    rand_optim = RandomizedSearchCV(model, param_distributions=params, \n",
    "                                          cv=5, random_state=42, n_jobs=-1, n_iter = 20, scoring='f1_micro')\n",
    "    rand_optim.fit(valid_x, valid_y)\n",
    "    cls = rand_optim.best_estimator_\n",
    "    cls.fit(train_x, train_y)\n",
    "    predict = cls.predict(test_x)\n",
    "    acc = metrics.accuracy_score(test_y, predict)\n",
    "    f1 = metrics.f1_score(test_y, predict, average='micro')\n",
    "    print(text, \": acc: \", acc,\" F1: \", f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NB(.......) :   acc:  0.4869950193691201  F1:  0.4869950193691201\n",
      "NB(e2v....) :   acc:  0.5383034231955095  F1:  0.5383034231955095\n",
      "NB(sen....) :   acc:  0.49055261285477114  F1:  0.49055261285477114\n",
      "NB(e2v+sen) :   acc:  0.5406751521859435  F1:  0.5406751521859435\n"
     ]
    }
   ],
   "source": [
    "experiment_classic(GaussianNB(), train_x, train_y, test_x, test_y, \"NB(.......)\")\n",
    "experiment_classic(GaussianNB(), train_x_e2v, train_y, test_x_e2v, test_y, \"NB(e2v....)\")\n",
    "experiment_classic(GaussianNB(), train_x_sen, train_y, test_x_sen, test_y, \"NB(sen....)\")\n",
    "experiment_classic(GaussianNB(), train_x_e2v_sen, train_y, test_x_e2v_sen, test_y, \"NB(e2v+sen)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RF(.......) : acc:  0.572377263024745  F1:  0.572377263024745\n",
      "RF(e2v....) : acc:  0.6494584552138509  F1:  0.6494584552138509\n",
      "RF(sen....) : acc:  0.6461380346272433  F1:  0.6461380346272433\n",
      "RF(e2v+sen) : acc:  0.683295122144043  F1:  0.683295122144043\n"
     ]
    }
   ],
   "source": [
    "params = {\n",
    "    'max_depth': stats.randint(1,15),\n",
    "    'criterion': ['gini', 'entropy'],\n",
    "    'min_samples_leaf': stats.randint(1,4),\n",
    "    'min_samples_split': stats.randint(2,5),\n",
    "    'n_estimators': [20,60,100,150]\n",
    "}\n",
    "experiment(RandomForestClassifier(random_state=0), params, train_x, train_y, valid_x, valid_y, test_x, test_y, \"RF(.......)\")\n",
    "experiment(RandomForestClassifier(random_state=0), params, train_x_e2v, train_y, valid_x_e2v, valid_y, test_x_e2v, test_y, \"RF(e2v....)\")\n",
    "experiment(RandomForestClassifier(random_state=0), params, train_x_sen, train_y, valid_x_sen, valid_y, test_x_sen, test_y, \"RF(sen....)\")\n",
    "experiment(RandomForestClassifier(random_state=0), params, train_x_e2v_sen, train_y, valid_x_e2v_sen, valid_y, test_x_e2v_sen, test_y, \"RF(e2v+sen)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GBT(.......) : acc:  0.6132500592932247  F1:  0.6132500592932247\n"
     ]
    }
   ],
   "source": [
    "params = {\n",
    "    'max_depth': stats.randint(1,15),\n",
    "    'learning_rate': [0.1,0.125,0.15,0.175,0.2,0.225,0.25],\n",
    "    'min_samples_leaf': stats.randint(1,5),\n",
    "    'min_samples_split' : stats.randint(2,10),\n",
    "    'n_estimators' : [20,60,100,150]\n",
    "}\n",
    "experiment(GradientBoostingClassifier(), params, train_x, train_y, valid_x, valid_y, test_x, test_y, \"GBT(.......)\")\n",
    "experiment(GradientBoostingClassifier(), params, train_x_e2v, train_y, valid_x_e2v, valid_y, test_x_e2v, test_y, \"GBT(e2v....)\")\n",
    "experiment(GradientBoostingClassifier(), params, train_x_sen, train_y, valid_x_sen, valid_y, test_x_sen, test_y, \"GBT(sen....)\")\n",
    "experiment(GradientBoostingClassifier(), params, train_x_e2v_sen, train_y, valid_x_e2v_sen, valid_y, test_x_e2v_sen, test_y, \"GBT(e2v+sen)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {\n",
    "    'kernel': ['poly', 'rbf', 'sigmoid','linear'],\n",
    "    'C': [1,1.25,1.5,1.75,2]\n",
    "}\n",
    "experiment(SVC(), params, train_x, train_y, valid_x, valid_y, test_x, test_y, \"SVM(.......)\")\n",
    "experiment(SVC(), params, train_x_e2v, train_y, valid_x_e2v, valid_y, test_x_e2v, test_y, \"SVM(e2v....)\")\n",
    "experiment(SVC(), params, train_x_sen, train_y, valid_x_sen, valid_y, test_x_sen, test_y, \"SVM(sen....)\")\n",
    "experiment(SVC(), params, train_x_e2v_sen, train_y, valid_x_e2v_sen, valid_y, test_x_e2v_sen, test_y, \"SVM(e2v+sen)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ulozenie chybovych dat na dalsiu analyzu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Programs\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_search.py:266: UserWarning: The total space of parameters 20 is smaller than n_iter=30. Running 20 iterations. For exhaustive searches, use GridSearchCV.\n",
      "  % (grid_size, self.n_iter, grid_size), UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "acc:  0.6339668914776211  F1:  0.5992901752670984\n"
     ]
    }
   ],
   "source": [
    "params = {\n",
    "    'kernel': ['poly', 'rbf', 'sigmoid','linear'],\n",
    "    'C': [1,1.25,1.5,1.75,2]\n",
    "}\n",
    "rand_optim = RandomizedSearchCV(SVC(), param_distributions=params, \n",
    "                                          cv=5, random_state=42, n_jobs=-1, n_iter = 20, scoring='f1_micro')\n",
    "rand_optim.fit(valid_x_e2v_sen, valid_y)\n",
    "cls = rand_optim.best_estimator_\n",
    "cls.fit(train_x_e2v_sen, train_y)\n",
    "predict = cls.predict(test_x_e2v_sen)\n",
    "acc = metrics.accuracy_score(test_y, predict)\n",
    "f1 = metrics.f1_score(test_y, predict, average='micro')\n",
    "print(\"acc: \", acc,\" F1: \", f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Different: 597 from all: 1631\n"
     ]
    }
   ],
   "source": [
    "predict_df = pd.DataFrame(data=predict,columns=['Pred'])\n",
    "compare_df = tweets[-len(predict_df):].reset_index(drop=True)\n",
    "compare_df = pd.concat([compare_df, predict_df], axis=1, sort=False)\n",
    "different = compare_df[compare_df['Label']!=compare_df['Pred']]\n",
    "different = different.reset_index(drop=True)\n",
    "print(f\"Different:\", len(different), \"from all:\", len(compare_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "different.to_csv('D:\\Downloads\\DP\\data\\errors_e2v.csv', index=False, encoding=\"utf-8-sig\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GradBoostTree ():  0.5789473684210527 0.5676951080390289\n"
     ]
    }
   ],
   "source": [
    "# grad. boosting trees\n",
    "model = GradientBoostingClassifier()\n",
    "model.fit(train_x, train_y)\n",
    "\n",
    "predict = model.predict(test_x)\n",
    "acc = metrics.accuracy_score(test_y, predict)\n",
    "f1 = metrics.f1_score(test_y, predict, average='micro')\n",
    "print(\"GradBoostTree (): \", acc, f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5676951080390289"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metrics.f1_score(test_y, predict, average='micro')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5789473684210527"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metrics.f1_score(test_y, predict, average='micro')\n",
    "# na stackoverflowe je pekne vysvetlene kedy a preco je to rovnake ako ACC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVM ():  0.6497929130234699 0.618961868646031\n"
     ]
    }
   ],
   "source": [
    "# SVM\n",
    "model = SVC(kernel='linear', C = 1.5)\n",
    "model.fit(train_x_e2v_sen, train_y)\n",
    "\n",
    "predict = model.predict(test_x_e2v_sen)\n",
    "acc = metrics.accuracy_score(test_y, predict)\n",
    "f1 = metrics.f1_score(test_y, predict, average='micro')\n",
    "print(\"SVM (): \", acc, f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RandomForest:  0.519558214450069 0.4884329060108204\n"
     ]
    }
   ],
   "source": [
    "# baseline\n",
    "model = RandomForestClassifier(n_estimators=60)\n",
    "model.fit(train_x, train_y)\n",
    "\n",
    "predict = model.predict(test_x)\n",
    "acc = metrics.accuracy_score(test_y, predict)\n",
    "f1 = metrics.f1_score(test_y, predict, average='micro')\n",
    "print(\"RandomForest: \", acc, f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import model_selection\n",
    "score = model_selection.cross_val_score(RandomForestClassifier(n_estimators=60), train_x, train_y, cv=5)\n",
    "print(score.mean(), score.std()*2)\n",
    "score = model_selection.cross_val_score(RandomForestClassifier(n_estimators=60), train_x_e2v, train_y, cv=5)\n",
    "print(score.mean(), score.std()*2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "limit = int(len(train_x)*0.8)\n",
    "valid_x = train_x[limit:]\n",
    "train_x = train_x[:limit]\n",
    "valid_y = train_y[limit:]\n",
    "train_y = train_y[:limit]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---RandomForest---\n",
      "Accuracy:  0.5773219814241486\n",
      "0.557900552605684\n",
      "\n",
      "<bound method BaseEstimator.get_params of RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
      "                       max_depth=14, max_features='auto', max_leaf_nodes=None,\n",
      "                       min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "                       min_samples_leaf=3, min_samples_split=4,\n",
      "                       min_weight_fraction_leaf=0.0, n_estimators=150,\n",
      "                       n_jobs=None, oob_score=False, random_state=0, verbose=0,\n",
      "                       warm_start=False)>\n"
     ]
    }
   ],
   "source": [
    "params = {\n",
    "    'max_depth': stats.randint(1,15),\n",
    "    'criterion': ['gini', 'entropy'],\n",
    "    'min_samples_leaf': stats.randint(1,4),\n",
    "    'min_samples_split': stats.randint(2,5),\n",
    "    'n_estimators': [20,60,100,150]\n",
    "}\n",
    "\n",
    "random_optimization = RandomizedSearchCV(RandomForestClassifier(random_state=0), param_distributions=params, \n",
    "                                          cv=5, random_state=42, n_jobs=-1, n_iter = 30, scoring='f1_micro')\n",
    "\n",
    "random_optimization.fit(valid_x, valid_y)\n",
    "\n",
    "\n",
    "# RandomForest\n",
    "cls = random_optimization.best_estimator_\n",
    "cls.fit(train_x, train_y)\n",
    "\n",
    "print(\"---RandomForest---\")         \n",
    "predict = cls.predict(test_x)\n",
    "print(\"Accuracy: \", metrics.accuracy_score(predict, test_y))\n",
    "print(metrics.f1_score(test_y, predict, average='micro'))\n",
    "print()\n",
    "print(cls.get_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
