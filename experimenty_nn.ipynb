{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from utils import encode_data_get_embeddings, create_emoji_sentiment\n",
    "import model as models\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, Bidirectional, LSTM, GRU, Dense\n",
    "from sklearn.metrics import f1_score, classification_report\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Nacitanie dat a embedingov"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tweets = pd.read_csv('D:/Downloads/DP/data/e2v_data.csv')\n",
    "tweets = pd.read_csv('D:/Downloads/DP/data/e2v_data_emoji.csv')\n",
    "\n",
    "# tweets = pd.read_csv('D:/Downloads/DP/data/emocontext.csv')\n",
    "# tweets = pd.read_csv('D:/Downloads/DP/data/emocontext_emoji.csv')\n",
    "# tweets_dev = pd.read_csv('D:/Downloads/DP/data/emocontext_dev.csv')\n",
    "# tweets_test = pd.read_csv('D:/Downloads/DP/data/emocontext_test.csv')\n",
    "# tw_cnt = len(tweets)\n",
    "# tweets = pd.concat([tweets,tweets_test])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2000000 300\n",
      "\n",
      "1661 300\n",
      "\n"
     ]
    }
   ],
   "source": [
    "w2v = open('D:\\Downloads\\DP\\word2vec300_en.vec', encoding=\"utf8\")\n",
    "w2v_dim = w2v.readline()\n",
    "print(w2v_dim)\n",
    "\n",
    "e2v = open('D:\\Downloads\\DP\\emoji2vec300.txt', encoding=\"utf8\")\n",
    "e2v_dim = e2v.readline()\n",
    "print(e2v_dim)\n",
    "\n",
    "train_x, train_y, test_x, test_y, embedding_matrix = encode_data_get_embeddings(tweets, 0.8, w2v, None)\n",
    "# train_x, train_y, test_x, test_y, embedding_matrix = encode_data_get_embeddings(tweets, tw_cnt, w2v, e2v)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predikcie - Neuronove siete"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BiLstm - emosent data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "classes = ['others', 'sad', 'happy', 'angry']\n",
    "predict = model.predict(test_x)\n",
    "predict = np.argmax(predict, axis=1)\n",
    "predict = [classes[pr] for pr in predict]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_2 (Embedding)      (None, 81, 300)           3668400   \n",
      "_________________________________________________________________\n",
      "bidirectional_3 (Bidirection (None, 512)               1140736   \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 1)                 513       \n",
      "=================================================================\n",
      "Total params: 4,809,649\n",
      "Trainable params: 1,141,249\n",
      "Non-trainable params: 3,668,400\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Embedding(embedding_matrix.shape[0],\n",
    "                 300,\n",
    "                 weights=[embedding_matrix],\n",
    "                 input_length=80,\n",
    "                 trainable=False))\n",
    "\n",
    "model.add(Bidirectional(LSTM(units=256,\n",
    "            dropout=0.1,\n",
    "            recurrent_dropout=0.1\n",
    "                            )))\n",
    "\n",
    "model.add(Dense(1, activation='softmax'))\n",
    "\n",
    "model.compile(loss = 'categorical_crossentropy', optimizer = 'adam', metrics = ['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "model.fit(train_x, train_y, \n",
    "          batch_size=64,\n",
    "          epochs=10,\n",
    "          validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5729836714497774\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       angry       0.54      0.80      0.65       298\n",
      "       happy       0.39      0.63      0.48       284\n",
      "      others       0.95      0.88      0.91      4677\n",
      "         sad       0.55      0.65      0.59       250\n",
      "\n",
      "    accuracy                           0.85      5509\n",
      "   macro avg       0.61      0.74      0.66      5509\n",
      "weighted avg       0.88      0.85      0.86      5509\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(f1_score(tweets.Label[tw_cnt:], predict, average='micro', labels=['sad', 'happy', 'angry']))\n",
    "print(classification_report(tweets.Label[tw_cnt:], predict))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BiLstm - e2v data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2000000 300\n",
      "\n",
      "1661 300\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tweets = pd.read_csv('D:/Downloads/DP/data/e2v_data_emoji.csv')\n",
    "\n",
    "w2v = open('D:\\Downloads\\DP\\word2vec300_en.vec', encoding=\"utf8\")\n",
    "w2v_dim = w2v.readline()\n",
    "print(w2v_dim)\n",
    "\n",
    "e2v = open('D:\\Downloads\\DP\\emoji2vec300.txt', encoding=\"utf8\")\n",
    "e2v_dim = e2v.readline()\n",
    "print(e2v_dim)\n",
    "\n",
    "train_x, train_y, test_x, test_y, embedding_matrix = encode_data_get_embeddings(tweets, 0.8, w2v, None)\n",
    "# train_x, train_y, test_x, test_y, embedding_matrix = encode_data_get_embeddings(tweets, 0.8, w2v, e2v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "109/109 [==============================] - 60s 546ms/step - loss: 0.9366 - acc: 0.5681 - val_loss: 0.9052 - val_acc: 0.5865\n",
      "Epoch 2/10\n",
      "109/109 [==============================] - 57s 522ms/step - loss: 0.8622 - acc: 0.5992 - val_loss: 0.8759 - val_acc: 0.5911\n",
      "Epoch 3/10\n",
      "109/109 [==============================] - 57s 521ms/step - loss: 0.8227 - acc: 0.6194 - val_loss: 0.8667 - val_acc: 0.5871\n",
      "Epoch 4/10\n",
      "109/109 [==============================] - 57s 521ms/step - loss: 0.7984 - acc: 0.6294 - val_loss: 0.8746 - val_acc: 0.5929\n",
      "Epoch 5/10\n",
      "109/109 [==============================] - 57s 526ms/step - loss: 0.7805 - acc: 0.6471 - val_loss: 0.8503 - val_acc: 0.5975\n",
      "Epoch 6/10\n",
      "109/109 [==============================] - 84s 773ms/step - loss: 0.7622 - acc: 0.6544 - val_loss: 0.8539 - val_acc: 0.6009\n",
      "Epoch 7/10\n",
      "109/109 [==============================] - 86s 787ms/step - loss: 0.7399 - acc: 0.6635 - val_loss: 0.8677 - val_acc: 0.6124\n",
      "Epoch 8/10\n",
      "109/109 [==============================] - 87s 794ms/step - loss: 0.7276 - acc: 0.6788 - val_loss: 0.8994 - val_acc: 0.5831\n",
      "Epoch 9/10\n",
      "109/109 [==============================] - 70s 641ms/step - loss: 0.7210 - acc: 0.6745 - val_loss: 0.8509 - val_acc: 0.5992\n",
      "Epoch 10/10\n",
      "109/109 [==============================] - 55s 502ms/step - loss: 0.6972 - acc: 0.6827 - val_loss: 0.8855 - val_acc: 0.5871\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x2e753b9e2e8>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = models.BiLstmModel(embedding_matrix)\n",
    "\n",
    "model.compile(\n",
    "    optimizer='adam',\n",
    "    loss='categorical_crossentropy',\n",
    "    metrics = ['acc'])  \n",
    "\n",
    "model.fit(\n",
    "    x=train_x,\n",
    "    y=train_y,\n",
    "    epochs=10,\n",
    "    batch_size=64,\n",
    "    validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 1.0444302558898926, acc: 0.5089737772941589\n"
     ]
    }
   ],
   "source": [
    "score = model.evaluate(test_x, test_y, verbose=0)\n",
    "print(f'loss: {score[0]}, acc: {score[1]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2000000 300\n",
      "\n",
      "1661 300\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tweets = pd.read_csv('D:/Downloads/DP/data/e2v_data_emoji.csv')\n",
    "\n",
    "w2v = open('D:\\Downloads\\DP\\word2vec300_en.vec', encoding=\"utf8\")\n",
    "w2v_dim = w2v.readline()\n",
    "print(w2v_dim)\n",
    "\n",
    "e2v = open('D:\\Downloads\\DP\\emoji2vec300.txt', encoding=\"utf8\")\n",
    "e2v_dim = e2v.readline()\n",
    "print(e2v_dim)\n",
    "\n",
    "# train_x, train_y, test_x, test_y, embedding_matrix = encode_data_get_embeddings(tweets, 0.8, w2v, None)\n",
    "train_x, train_y, test_x, test_y, embedding_matrix = encode_data_get_embeddings(tweets, 0.8, w2v, e2v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "109/109 [==============================] - 61s 558ms/step - loss: 0.8700 - acc: 0.5972 - val_loss: 0.8776 - val_acc: 0.6113\n",
      "Epoch 2/10\n",
      "109/109 [==============================] - 63s 576ms/step - loss: 0.8078 - acc: 0.6407 - val_loss: 0.8673 - val_acc: 0.6107\n",
      "Epoch 3/10\n",
      "109/109 [==============================] - 64s 585ms/step - loss: 0.7755 - acc: 0.6494 - val_loss: 0.8542 - val_acc: 0.6182\n",
      "Epoch 4/10\n",
      "109/109 [==============================] - 63s 578ms/step - loss: 0.7425 - acc: 0.6689 - val_loss: 0.8517 - val_acc: 0.6055\n",
      "Epoch 5/10\n",
      "109/109 [==============================] - 63s 579ms/step - loss: 0.7100 - acc: 0.6886 - val_loss: 0.8691 - val_acc: 0.6164\n",
      "Epoch 6/10\n",
      "109/109 [==============================] - 64s 583ms/step - loss: 0.6741 - acc: 0.6991 - val_loss: 0.8722 - val_acc: 0.6216\n",
      "Epoch 7/10\n",
      "109/109 [==============================] - 84s 770ms/step - loss: 0.6452 - acc: 0.7229 - val_loss: 0.9217 - val_acc: 0.6049\n",
      "Epoch 8/10\n",
      "109/109 [==============================] - 96s 884ms/step - loss: 0.6084 - acc: 0.7382 - val_loss: 0.9319 - val_acc: 0.6021\n",
      "Epoch 9/10\n",
      "109/109 [==============================] - 96s 881ms/step - loss: 0.5780 - acc: 0.7568 - val_loss: 0.9657 - val_acc: 0.6049\n",
      "Epoch 10/10\n",
      "109/109 [==============================] - 76s 696ms/step - loss: 0.5324 - acc: 0.7735 - val_loss: 1.0272 - val_acc: 0.5929\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x2e71d19b6d8>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = models.BiLstmModel(embedding_matrix)\n",
    "\n",
    "model.compile(\n",
    "    optimizer='adam',\n",
    "    loss='categorical_crossentropy',\n",
    "    metrics = ['acc'])  \n",
    "\n",
    "model.fit(\n",
    "    x=train_x,\n",
    "    y=train_y,\n",
    "    epochs=10,\n",
    "    batch_size=64,\n",
    "    validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 0.9752941727638245, acc: 0.6000920534133911\n"
     ]
    }
   ],
   "source": [
    "score = model.evaluate(test_x, test_y, verbose=0)\n",
    "print(f'loss: {score[0]}, acc: {score[1]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2000000 300\n",
      "\n",
      "1661 300\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tweets = pd.read_csv('D:/Downloads/DP/data/e2v_data.csv')\n",
    "\n",
    "w2v = open('D:\\Downloads\\DP\\word2vec300_en.vec', encoding=\"utf8\")\n",
    "w2v_dim = w2v.readline()\n",
    "print(w2v_dim)\n",
    "\n",
    "e2v = open('D:\\Downloads\\DP\\emoji2vec300.txt', encoding=\"utf8\")\n",
    "e2v_dim = e2v.readline()\n",
    "print(e2v_dim)\n",
    "\n",
    "train_x, train_y, test_x, test_y, embedding_matrix = encode_data_get_embeddings(tweets, 0.8, w2v, None)\n",
    "# train_x, train_y, test_x, test_y, embedding_matrix = encode_data_get_embeddings(tweets, 0.8, w2v, e2v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "646/646 [==============================] - 381s 590ms/step - loss: 0.8908 - acc: 0.5858 - val_loss: 0.8307 - val_acc: 0.6277\n",
      "Epoch 2/10\n",
      "646/646 [==============================] - 397s 614ms/step - loss: 0.8177 - acc: 0.6306 - val_loss: 0.8071 - val_acc: 0.6319\n",
      "Epoch 3/10\n",
      "646/646 [==============================] - 396s 613ms/step - loss: 0.7895 - acc: 0.6466 - val_loss: 0.7871 - val_acc: 0.6502\n",
      "Epoch 4/10\n",
      "646/646 [==============================] - 397s 615ms/step - loss: 0.7725 - acc: 0.6556 - val_loss: 0.7838 - val_acc: 0.6481\n",
      "Epoch 5/10\n",
      "646/646 [==============================] - 437s 677ms/step - loss: 0.7561 - acc: 0.6640 - val_loss: 0.7766 - val_acc: 0.6553\n",
      "Epoch 6/10\n",
      "646/646 [==============================] - 394s 610ms/step - loss: 0.7404 - acc: 0.6713 - val_loss: 0.7789 - val_acc: 0.6529\n",
      "Epoch 7/10\n",
      "646/646 [==============================] - 390s 604ms/step - loss: 0.7226 - acc: 0.6811 - val_loss: 0.7774 - val_acc: 0.6573\n",
      "Epoch 8/10\n",
      "646/646 [==============================] - 392s 607ms/step - loss: 0.7021 - acc: 0.6922 - val_loss: 0.7928 - val_acc: 0.6489\n",
      "Epoch 9/10\n",
      "646/646 [==============================] - 391s 605ms/step - loss: 0.6759 - acc: 0.7042 - val_loss: 0.8025 - val_acc: 0.6479\n",
      "Epoch 10/10\n",
      "646/646 [==============================] - 409s 634ms/step - loss: 0.6416 - acc: 0.7201 - val_loss: 0.8248 - val_acc: 0.6493\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x2e75398b3c8>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = models.BiLstmModel(embedding_matrix)\n",
    "\n",
    "model.compile(\n",
    "    optimizer='adam',\n",
    "    loss='categorical_crossentropy',\n",
    "    metrics = ['acc'])  \n",
    "\n",
    "model.fit(\n",
    "    x=train_x,\n",
    "    y=train_y,\n",
    "    epochs=10,\n",
    "    batch_size=64,\n",
    "    validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 0.8660539388656616, acc: 0.6269350051879883\n"
     ]
    }
   ],
   "source": [
    "score = model.evaluate(test_x, test_y, verbose=0)\n",
    "print(f'loss: {score[0]}, acc: {score[1]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2000000 300\n",
      "\n",
      "1661 300\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tweets = pd.read_csv('D:/Downloads/DP/data/e2v_data.csv')\n",
    "\n",
    "w2v = open('D:\\Downloads\\DP\\word2vec300_en.vec', encoding=\"utf8\")\n",
    "w2v_dim = w2v.readline()\n",
    "print(w2v_dim)\n",
    "\n",
    "e2v = open('D:\\Downloads\\DP\\emoji2vec300.txt', encoding=\"utf8\")\n",
    "e2v_dim = e2v.readline()\n",
    "print(e2v_dim)\n",
    "\n",
    "# train_x, train_y, test_x, test_y, embedding_matrix = encode_data_get_embeddings(tweets, 0.8, w2v, None)\n",
    "train_x, train_y, test_x, test_y, embedding_matrix = encode_data_get_embeddings(tweets, 0.8, w2v, e2v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "646/646 [==============================] - 381s 589ms/step - loss: 0.8845 - acc: 0.5900 - val_loss: 0.8399 - val_acc: 0.6203\n",
      "Epoch 2/10\n",
      "646/646 [==============================] - 352s 545ms/step - loss: 0.8115 - acc: 0.6377 - val_loss: 0.8081 - val_acc: 0.6416\n",
      "Epoch 3/10\n",
      "646/646 [==============================] - 401s 621ms/step - loss: 0.7826 - acc: 0.6514 - val_loss: 0.7883 - val_acc: 0.6507\n",
      "Epoch 4/10\n",
      "646/646 [==============================] - 343s 531ms/step - loss: 0.7618 - acc: 0.6623 - val_loss: 0.7823 - val_acc: 0.6554\n",
      "Epoch 5/10\n",
      "646/646 [==============================] - 354s 547ms/step - loss: 0.7452 - acc: 0.6706 - val_loss: 0.7724 - val_acc: 0.6611\n",
      "Epoch 6/10\n",
      "646/646 [==============================] - 397s 614ms/step - loss: 0.7262 - acc: 0.6793 - val_loss: 0.7797 - val_acc: 0.6578\n",
      "Epoch 7/10\n",
      "646/646 [==============================] - 341s 527ms/step - loss: 0.7069 - acc: 0.6902 - val_loss: 0.7801 - val_acc: 0.6564\n",
      "Epoch 8/10\n",
      "646/646 [==============================] - 346s 536ms/step - loss: 0.6857 - acc: 0.7008 - val_loss: 0.7941 - val_acc: 0.6538\n",
      "Epoch 9/10\n",
      "646/646 [==============================] - 351s 544ms/step - loss: 0.6615 - acc: 0.7118 - val_loss: 0.8235 - val_acc: 0.6449\n",
      "Epoch 10/10\n",
      "646/646 [==============================] - 397s 614ms/step - loss: 0.6313 - acc: 0.7270 - val_loss: 0.8388 - val_acc: 0.6478\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x2239dc9c160>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = models.BiLstmModel(embedding_matrix)\n",
    "\n",
    "model.compile(\n",
    "    optimizer='adam',\n",
    "    loss='categorical_crossentropy',\n",
    "    metrics = ['acc'])  \n",
    "\n",
    "model.fit(\n",
    "    x=train_x,\n",
    "    y=train_y,\n",
    "    epochs=10,\n",
    "    batch_size=64,\n",
    "    validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 0.851948618888855, acc: 0.6384674906730652\n"
     ]
    }
   ],
   "source": [
    "score = model.evaluate(test_x, test_y, verbose=0)\n",
    "print(f'loss: {score[0]}, acc: {score[1]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BiLstm model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "302/302 [==============================] - 117s 389ms/step - loss: 0.8910 - acc: 0.6539 - val_loss: 0.6864 - val_acc: 0.7261\n",
      "Epoch 2/10\n",
      "302/302 [==============================] - 104s 343ms/step - loss: 0.5986 - acc: 0.7704 - val_loss: 0.5821 - val_acc: 0.7793\n",
      "Epoch 3/10\n",
      "302/302 [==============================] - 105s 348ms/step - loss: 0.5073 - acc: 0.8099 - val_loss: 0.5337 - val_acc: 0.7984\n",
      "Epoch 4/10\n",
      "302/302 [==============================] - 105s 348ms/step - loss: 0.4685 - acc: 0.8220 - val_loss: 0.5300 - val_acc: 0.8040\n",
      "Epoch 5/10\n",
      "302/302 [==============================] - 105s 349ms/step - loss: 0.4275 - acc: 0.8405 - val_loss: 0.4784 - val_acc: 0.8257\n",
      "Epoch 6/10\n",
      "302/302 [==============================] - 106s 350ms/step - loss: 0.3968 - acc: 0.8557 - val_loss: 0.4467 - val_acc: 0.8361\n",
      "Epoch 7/10\n",
      "302/302 [==============================] - 106s 350ms/step - loss: 0.3733 - acc: 0.8645 - val_loss: 0.4531 - val_acc: 0.8373\n",
      "Epoch 8/10\n",
      "302/302 [==============================] - 106s 350ms/step - loss: 0.3477 - acc: 0.8734 - val_loss: 0.4665 - val_acc: 0.8272\n",
      "Epoch 9/10\n",
      "302/302 [==============================] - 106s 351ms/step - loss: 0.3281 - acc: 0.8802 - val_loss: 0.4355 - val_acc: 0.8458\n",
      "Epoch 10/10\n",
      "302/302 [==============================] - 106s 352ms/step - loss: 0.3099 - acc: 0.8880 - val_loss: 0.4408 - val_acc: 0.8433\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x2840e895f60>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = models.BiLstmModel(embedding_matrix)\n",
    "\n",
    "model.compile(\n",
    "    optimizer='adam',\n",
    "    loss='categorical_crossentropy',\n",
    "    metrics = ['acc'])  \n",
    "\n",
    "model.fit(\n",
    "    x=train_x,\n",
    "    y=train_y,\n",
    "    epochs=10,\n",
    "    batch_size=64,\n",
    "    validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 0.430917888879776, acc: 0.8461538553237915\n"
     ]
    }
   ],
   "source": [
    "score = model.evaluate(test_x, test_y, verbose=0)\n",
    "print(f'loss: {score[0]}, acc: {score[1]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "model = models.TestModel(embedding_matrix)\n",
    "\n",
    "model.compile(\n",
    "    optimizer='adam',\n",
    "    loss='binary_crossentropy',\n",
    "    metrics = ['acc'])  \n",
    "\n",
    "model.fit(\n",
    "    x=train_x,\n",
    "    y=train_y,\n",
    "    epochs=10,\n",
    "    batch_size=32,\n",
    "    validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score = model.evaluate(test_x, test_y, verbose=0)\n",
    "print(f'loss: {score[0]}, acc: {score[1]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Combi model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "1292/1292 [==============================] - 554s 429ms/step - loss: 0.0798 - acc: 0.4610 - val_loss: 0.5632 - val_acc: 0.4553\n",
      "Epoch 2/10\n",
      "1292/1292 [==============================] - 545s 421ms/step - loss: -0.2002 - acc: 0.4619 - val_loss: 0.0506 - val_acc: 0.4574\n",
      "Epoch 3/10\n",
      "1292/1292 [==============================] - 564s 437ms/step - loss: -0.0424 - acc: 0.4650 - val_loss: -0.2589 - val_acc: 0.4577\n",
      "Epoch 4/10\n",
      "1292/1292 [==============================] - 583s 451ms/step - loss: -0.2362 - acc: 0.4639 - val_loss: -0.1299 - val_acc: 0.4602\n",
      "Epoch 5/10\n",
      "1292/1292 [==============================] - 572s 443ms/step - loss: -0.8435 - acc: 0.4725 - val_loss: -1.1468 - val_acc: 0.5018\n",
      "Epoch 6/10\n",
      "1292/1292 [==============================] - 569s 440ms/step - loss: -1.1712 - acc: 0.4839 - val_loss: -1.3310 - val_acc: 0.4689\n",
      "Epoch 7/10\n",
      "1292/1292 [==============================] - 540s 418ms/step - loss: -0.8657 - acc: 0.4749 - val_loss: -1.2684 - val_acc: 0.4702\n",
      "Epoch 8/10\n",
      "1292/1292 [==============================] - 531s 411ms/step - loss: -1.3547 - acc: 0.4917 - val_loss: -0.9495 - val_acc: 0.4938\n",
      "Epoch 9/10\n",
      "1292/1292 [==============================] - 585s 453ms/step - loss: -1.4537 - acc: 0.5080 - val_loss: -1.5250 - val_acc: 0.5150\n",
      "Epoch 10/10\n",
      "1292/1292 [==============================] - 555s 429ms/step - loss: -1.4946 - acc: 0.5108 - val_loss: -1.4480 - val_acc: 0.5141\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x29ba122e4a8>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = models.BiLstmModel(embedding_matrix)\n",
    "\n",
    "model.compile(\n",
    "    optimizer='adam',\n",
    "    loss='binary_crossentropy',\n",
    "    metrics = ['acc'])  \n",
    "\n",
    "model.fit(\n",
    "    x=train_x,\n",
    "    y=train_y,\n",
    "    epochs=10,\n",
    "    batch_size=32,\n",
    "    validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: -1.3393548727035522, acc: 0.5148705244064331\n"
     ]
    }
   ],
   "source": [
    "score = model.evaluate(test_x, test_y, verbose=0)\n",
    "print(f'loss: {score[0]}, acc: {score[1]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conv model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "1292/1292 [==============================] - 89s 69ms/step - loss: -988483.5000 - acc: 0.4483 - val_loss: -4403449.0000 - val_acc: 0.4309\n",
      "Epoch 2/5\n",
      "1292/1292 [==============================] - 87s 67ms/step - loss: -27949140.0000 - acc: 0.4437 - val_loss: -57549448.0000 - val_acc: 0.4421\n",
      "Epoch 3/5\n",
      "1292/1292 [==============================] - 87s 67ms/step - loss: -149946816.0000 - acc: 0.4436 - val_loss: -222597328.0000 - val_acc: 0.4482\n",
      "Epoch 4/5\n",
      "1292/1292 [==============================] - 95s 74ms/step - loss: -460484768.0000 - acc: 0.4442 - val_loss: -571984832.0000 - val_acc: 0.4502\n",
      "Epoch 5/5\n",
      "1292/1292 [==============================] - 105s 81ms/step - loss: -1016310080.0000 - acc: 0.4449 - val_loss: -1172016640.0000 - val_acc: 0.4497\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x29bb4353470>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = models.ConvModel(embedding_matrix)\n",
    "\n",
    "model.compile(\n",
    "    optimizer='adam',\n",
    "    loss='binary_crossentropy',\n",
    "    metrics = ['acc'])  \n",
    "\n",
    "model.fit(\n",
    "    x=train_x,\n",
    "    y=train_y,\n",
    "    epochs=5,\n",
    "    batch_size=32,\n",
    "    validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: -1159315328.0, acc: 0.45410215854644775\n"
     ]
    }
   ],
   "source": [
    "score = model.evaluate(test_x, test_y, verbose=0)\n",
    "print(f'loss: {score[0]}, acc: {score[1]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Baseline model\n",
    "- e2v data: 46% (with/out e2v)\n",
    "- e2v_emoji data: 25% (with/out e2v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "3679/3679 [==============================] - 87s 24ms/step - loss: 0.1552 - acc: 0.1649 - val_loss: 0.1269 - val_acc: 0.1637\n",
      "Epoch 2/5\n",
      "3679/3679 [==============================] - 83s 23ms/step - loss: 0.1453 - acc: 0.1624 - val_loss: 0.1179 - val_acc: 0.1631\n",
      "Epoch 3/5\n",
      "3679/3679 [==============================] - 83s 23ms/step - loss: 0.1310 - acc: 0.1613 - val_loss: 0.1196 - val_acc: 0.1630\n",
      "Epoch 4/5\n",
      "3679/3679 [==============================] - 83s 23ms/step - loss: 0.1287 - acc: 0.1612 - val_loss: 0.1182 - val_acc: 0.1630\n",
      "Epoch 5/5\n",
      "3679/3679 [==============================] - 83s 23ms/step - loss: 0.1291 - acc: 0.1612 - val_loss: 0.1175 - val_acc: 0.1630\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x1b12d75d390>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = models.BaselineModel(embedding_matrix)\n",
    "\n",
    "model.compile(\n",
    "    optimizer='adam',\n",
    "    loss='binary_crossentropy',\n",
    "    metrics = ['acc'])  \n",
    "\n",
    "model.fit(\n",
    "    x=train_x,\n",
    "    y=train_y,\n",
    "    epochs=5,\n",
    "    batch_size=32,\n",
    "    validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 0.10143047571182251, acc: 0.1616787612438202\n"
     ]
    }
   ],
   "source": [
    "score = model.evaluate(test_x, test_y, verbose=0)\n",
    "print(f'loss: {score[0]}, acc: {score[1]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BaselineDrop model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "1292/1292 [==============================] - 35s 27ms/step - loss: 0.1600 - acc: 0.4608 - val_loss: 0.1721 - val_acc: 0.4550\n",
      "Epoch 2/5\n",
      "1292/1292 [==============================] - 35s 27ms/step - loss: 0.1503 - acc: 0.4612 - val_loss: 0.1794 - val_acc: 0.4550\n",
      "Epoch 3/5\n",
      "1292/1292 [==============================] - 36s 28ms/step - loss: 0.1530 - acc: 0.4610 - val_loss: 0.1714 - val_acc: 0.4569\n",
      "Epoch 4/5\n",
      "1292/1292 [==============================] - 40s 31ms/step - loss: 0.1960 - acc: 0.4572 - val_loss: 0.3097 - val_acc: 0.4471\n",
      "Epoch 5/5\n",
      "1292/1292 [==============================] - 41s 31ms/step - loss: 0.2088 - acc: 0.4567 - val_loss: 0.1994 - val_acc: 0.4549\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x29b9ee9a588>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = models.BaselineDropModel(embedding_matrix)\n",
    "\n",
    "model.compile(\n",
    "    optimizer='adam',\n",
    "    loss='binary_crossentropy',\n",
    "    metrics = ['acc'])  \n",
    "\n",
    "model.fit(\n",
    "    x=train_x,\n",
    "    y=train_y,\n",
    "    epochs=5,\n",
    "    batch_size=32,\n",
    "    validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 0.18892277777194977, acc: 0.46122291684150696\n"
     ]
    }
   ],
   "source": [
    "score = model.evaluate(test_x, test_y, verbose=0)\n",
    "print(f'loss: {score[0]}, acc: {score[1]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lstm model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "109/109 [==============================] - 13s 115ms/step - loss: -1.0244 - acc: 0.3762 - val_loss: -1.5107 - val_acc: 0.3341\n",
      "Epoch 2/10\n",
      "109/109 [==============================] - 11s 98ms/step - loss: -1.4884 - acc: 0.4302 - val_loss: -1.5820 - val_acc: 0.4598\n",
      "Epoch 3/10\n",
      "109/109 [==============================] - 11s 100ms/step - loss: -1.6164 - acc: 0.3240 - val_loss: -1.6535 - val_acc: 0.3059\n",
      "Epoch 4/10\n",
      "109/109 [==============================] - 10s 95ms/step - loss: -1.2929 - acc: 0.3465 - val_loss: -1.1082 - val_acc: 0.4153\n",
      "Epoch 5/10\n",
      "109/109 [==============================] - 11s 100ms/step - loss: -1.4010 - acc: 0.3545 - val_loss: -1.5625 - val_acc: 0.2516\n",
      "Epoch 6/10\n",
      "109/109 [==============================] - 11s 100ms/step - loss: -1.2555 - acc: 0.2757 - val_loss: -1.3604 - val_acc: 0.2708\n",
      "Epoch 7/10\n",
      "109/109 [==============================] - 11s 103ms/step - loss: -1.3701 - acc: 0.2894 - val_loss: -1.6187 - val_acc: 0.2590\n",
      "Epoch 8/10\n",
      "109/109 [==============================] - 11s 99ms/step - loss: -1.5024 - acc: 0.2810 - val_loss: -1.5444 - val_acc: 0.2541\n",
      "Epoch 9/10\n",
      "109/109 [==============================] - 10s 96ms/step - loss: -1.5518 - acc: 0.2792 - val_loss: -1.6843 - val_acc: 0.2563\n",
      "Epoch 10/10\n",
      "109/109 [==============================] - 10s 95ms/step - loss: -1.6163 - acc: 0.2811 - val_loss: -1.5885 - val_acc: 0.2684\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x1ff484414e0>"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = models.LstmModel(embedding_matrix)\n",
    "\n",
    "model.compile(\n",
    "    optimizer='adam',\n",
    "    loss='binary_crossentropy',\n",
    "    metrics = ['acc'])  \n",
    "\n",
    "model.fit(\n",
    "    x=train_x,\n",
    "    y=train_y,\n",
    "    epochs=10,\n",
    "    batch_size=64,\n",
    "    validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: -1.4610261917114258, acc: 0.2863207757472992\n"
     ]
    }
   ],
   "source": [
    "score = model.evaluate(test_x, test_y, verbose=0)\n",
    "print(f'loss: {score[0]}, acc: {score[1]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(101147, 300)"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding_matrix.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(12228, 300)"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding_matrix.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import numpy as np\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "\n",
    "tw_tok = TweetTokenizer(preserve_case=False, reduce_len=True, strip_handles=True)\n",
    "\n",
    "x = np.array((tweets['Text']).apply(lambda x: ' '.join([w for w in tw_tok.tokenize(x)])))\n",
    "\n",
    "# One-hot encoding labelu\n",
    "y = np.array(tweets['Label'].apply(lambda x: 2 if x == 'Positive' else (1 if x =='Neutral' else 0)))\n",
    "#     y = np.array(tweets['Label'].apply(lambda x: 1 if x == 'Positive' else 0))\n",
    "\n",
    "tweet_count = len(tweets)\n",
    "limit = int(0.8 * tweet_count)\n",
    "\n",
    "train_x = x[:limit]\n",
    "train_y = y[:limit]\n",
    "test_x = x[limit:]\n",
    "test_y = y[limit:]\n",
    "\n",
    "tokenizer = Tokenizer(num_words=5000)\n",
    "tokenizer.fit_on_texts(train_x)\n",
    "\n",
    "train_x = tokenizer.texts_to_sequences(train_x)\n",
    "test_x = tokenizer.texts_to_sequences(test_x)\n",
    "\n",
    "vocab_size = len(tokenizer.word_index) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "59945"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "59991"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding_matrix.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
