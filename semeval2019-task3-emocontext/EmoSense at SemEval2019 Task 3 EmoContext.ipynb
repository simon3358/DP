{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EmoSense at SemEval-2019 Task 3: Bidirectional LSTM Network for Contextual Emotion Detection in Textual Conversations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Loading Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Programs\\Anaconda3\\lib\\site-packages\\ekphrasis\\classes\\tokenizer.py:225: FutureWarning: Possible nested set at position 2190\n",
      "  self.tok = re.compile(r\"({})\".format(\"|\".join(pipeline)))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading twitter - 1grams ...\n",
      "Reading twitter - 2grams ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Programs\\Anaconda3\\lib\\site-packages\\ekphrasis\\classes\\exmanager.py:14: FutureWarning: Possible nested set at position 42\n",
      "  regexes = {k.lower(): re.compile(self.expressions[k]) for k, v in\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading twitter - 1grams ...\n"
     ]
    }
   ],
   "source": [
    "from ekphrasis.classes.preprocessor import TextPreProcessor\n",
    "from ekphrasis.classes.tokenizer import SocialTokenizer\n",
    "from ekphrasis.dicts.emoticons import emoticons\n",
    "import numpy as np\n",
    "\n",
    "import re\n",
    "import io\n",
    "\n",
    "label2emotion = {0: \"others\", 1: \"happy\", 2: \"sad\", 3: \"angry\"}\n",
    "emotion2label = {\"others\": 0, \"happy\": 1, \"sad\": 2, \"angry\": 3}\n",
    "\n",
    "emoticons_additional = {\n",
    "    '(^・^)': '<happy>', ':‑c': '<sad>', '=‑d': '<happy>', \":'‑)\": '<happy>', ':‑d': '<laugh>',\n",
    "    ':‑(': '<sad>', ';‑)': '<happy>', ':‑)': '<happy>', ':\\\\/': '<sad>', 'd=<': '<annoyed>',\n",
    "    ':‑/': '<annoyed>', ';‑]': '<happy>', '(^�^)': '<happy>', 'angru': 'angry', \"d‑':\":\n",
    "        '<annoyed>', \":'‑(\": '<sad>', \":‑[\": '<annoyed>', '(�?�)': '<happy>', 'x‑d': '<laugh>',\n",
    "}\n",
    "\n",
    "text_processor = TextPreProcessor(\n",
    "    # terms that will be normalized\n",
    "    normalize=['url', 'email', 'percent', 'money', 'phone', 'user',\n",
    "               'time', 'url', 'date', 'number'],\n",
    "    # terms that will be annotated\n",
    "    annotate={\"hashtag\", \"allcaps\", \"elongated\", \"repeated\",\n",
    "              'emphasis', 'censored'},\n",
    "    fix_html=True,  # fix HTML tokens\n",
    "    # corpus from which the word statistics are going to be used \n",
    "    # for word segmentation \n",
    "    segmenter=\"twitter\",\n",
    "    # corpus from which the word statistics are going to be used \n",
    "    # for spell correction\n",
    "    corrector=\"twitter\",\n",
    "    unpack_hashtags=True,  # perform word segmentation on hashtags\n",
    "    unpack_contractions=True,  # Unpack contractions (can't -> can not)\n",
    "    spell_correct_elong=True,  # spell correction for elongated words\n",
    "    # select a tokenizer. You can use SocialTokenizer, or pass your own\n",
    "    # the tokenizer, should take as input a string and return a list of tokens\n",
    "    tokenizer=SocialTokenizer(lowercase=True).tokenize,\n",
    "    # list of dictionaries, for replacing tokens extracted from the text,\n",
    "    # with other expressions. You can pass more than one dictionaries.\n",
    "    dicts=[emoticons, emoticons_additional]\n",
    ")\n",
    "\n",
    "\n",
    "def tokenize(text):\n",
    "    text = \" \".join(text_processor.pre_process_doc(text))\n",
    "    return text\n",
    "\n",
    "\n",
    "def preprocessData(dataFilePath, mode):\n",
    "    conversations = []\n",
    "    labels = []\n",
    "    with io.open(dataFilePath, encoding=\"utf8\") as finput:\n",
    "        finput.readline()\n",
    "        for line in finput:\n",
    "            line = line.strip().split('\\t')\n",
    "            for i in range(1, 4):\n",
    "                line[i] = tokenize(line[i])\n",
    "            if mode == \"train\":\n",
    "                labels.append(emotion2label[line[4]])\n",
    "            conv = line[1:4]\n",
    "            conversations.append(conv)\n",
    "    if mode == \"train\":\n",
    "        return np.array(conversations), np.array(labels)\n",
    "    else:\n",
    "        return np.array(conversations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts_train, labels_train = preprocessData('data/train.txt', mode=\"train\")\n",
    "texts_dev, labels_dev = preprocessData('data/dev.txt', mode=\"train\")\n",
    "texts_test, labels_test = preprocessData('data/test.txt', mode=\"train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "emo_mapping = {\n",
    "    '🙂':'',\n",
    "    '🙁':'😕',\n",
    "    '🤣':'😂',\n",
    "    '🤐':'😬',\n",
    "    '🙄':'😏',\n",
    "    '🍾':'🍹',\n",
    "    '🤗':'☺',\n",
    "    '🤔':'😏',\n",
    "    '🤡':'🃏',\n",
    "    '🛰':'',\n",
    "    '🤑':'💰',\n",
    "    '\\u200d':'',\n",
    "    '🤥':'😢',\n",
    "    '🤕':'',\n",
    "    '🖕':'',\n",
    "    '🤦':'',\n",
    "    '🕺':'',\n",
    "    '🏕':'',\n",
    "    '🙃':'',\n",
    "    '🤒':'',\n",
    "    '🏣':'',\n",
    "    '🤷':'💁',\n",
    "    '🤢':'',\n",
    "    '🏖':'',\n",
    "   '🏋':'',\n",
    "    '🤘':'',\n",
    "    '🤖':'',\n",
    "    '⏸':''\n",
    "}\n",
    "\n",
    "for x in texts_train:\n",
    "    for y in x:\n",
    "        for i, j in emo_mapping.items():\n",
    "            y = y.replace(i, j)\n",
    "            \n",
    "for x in texts_dev:\n",
    "    for y in x:\n",
    "        for i, j in emo_mapping.items():\n",
    "            y = y.replace(i, j)\n",
    "            \n",
    "for x in texts_test:\n",
    "    for y in x:\n",
    "        for i, j in emo_mapping.items():\n",
    "            y = y.replace(i, j)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([['do you dance ?', 'yes i love to dance 😻',\n",
       "        '😂 😂 😂 so you have legs too'],\n",
       "       ['i hate it too', 'guess what , i do not .', 'even i do not'],\n",
       "       ['not always', 'what about yesterday',\n",
       "        'do u know what <number> is ?'],\n",
       "       ['bcoz u dont know wat is to miss someone',\n",
       "        'but sometimes one can not express the same', '😢'],\n",
       "       ['yeah', 'i will ask around', 'which is your favourite movie']],\n",
       "      dtype='<U625')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "texts_train[20:25]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Loading Word Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getEmbeddings(file):\n",
    "    embeddingsIndex = {}\n",
    "    with io.open(file, encoding=\"utf8\") as f:\n",
    "        for line in f:\n",
    "            values = line.split()\n",
    "            word = values[0]\n",
    "            embeddingVector = np.asarray(values[1:], dtype='float32')\n",
    "            embeddingsIndex[word] = embeddingVector \n",
    "            \n",
    "    iter = 0\n",
    "    with io.open('D:\\Downloads\\DP\\emoji2vec300.txt', encoding=\"utf8\") as f:\n",
    "        for line in f:\n",
    "            if iter == 0:\n",
    "                iter += 1\n",
    "                continue\n",
    "            values = line.split()\n",
    "            word = values[0]\n",
    "            embeddingVector = np.asarray(values[1:], dtype='float32')\n",
    "            embeddingsIndex[word] = embeddingVector \n",
    "            \n",
    "    return embeddingsIndex, 300\n",
    "\n",
    "\n",
    "def getEmbeddingMatrix(wordIndex, embeddings, dim):\n",
    "    embeddingMatrix = np.zeros((len(wordIndex) + 1, dim))\n",
    "    for word, i in wordIndex.items():\n",
    "        embeddingMatrix[i] = embeddings.get(word)\n",
    "    return embeddingMatrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 658845 unique tokens.\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "\n",
    "embeddings, dim = getEmbeddings('D:\\Downloads\\DP\\emosense.300d.txt')\n",
    "tokenizer = Tokenizer(filters='')\n",
    "tokenizer.fit_on_texts([' '.join(list(embeddings.keys()))])\n",
    "\n",
    "wordIndex = tokenizer.word_index\n",
    "print(\"Found %s unique tokens.\" % len(wordIndex))\n",
    "\n",
    "embeddings_matrix = getEmbeddingMatrix(wordIndex, embeddings, dim) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Texts Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from emosent import get_emoji_sentiment_rank\n",
    "\n",
    "\n",
    "MAX_SEQUENCE_LENGTH = 24\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(texts_train, labels_train, test_size=0.2, random_state=42)\n",
    "\n",
    "labels_categorical_train = to_categorical(np.asarray(y_train))\n",
    "labels_categorical_val = to_categorical(np.asarray(y_val))\n",
    "labels_categorical_dev = to_categorical(np.asarray(labels_dev))\n",
    "labels_categorical_test = to_categorical(np.asarray(labels_test))\n",
    "\n",
    "\n",
    "def add_lexical_feature(texts, message, seq):\n",
    "    sent_arr = np.zeros((len(message),1))   # create array of zeros\n",
    "    for i in range(0, len(message)):\n",
    "        sent = 0\n",
    "        l = 0\n",
    "        for char in texts[i][seq]:\n",
    "            try:\n",
    "                sent += get_emoji_sentiment_rank(char)[\"sentiment_score\"] \n",
    "                l+=1\n",
    "            except:\n",
    "                pass\n",
    "        if l>0:\n",
    "            sent /= l    \n",
    "        sent_arr[i] = sent\n",
    "\n",
    "    message = np.append(message, sent_arr, axis=1)\n",
    "    return message\n",
    "    \n",
    "\n",
    "def get_sequances(texts, sequence_length):\n",
    "    message_first = pad_sequences(tokenizer.texts_to_sequences(texts[:, 0]), sequence_length)\n",
    "    message_second = pad_sequences(tokenizer.texts_to_sequences(texts[:, 1]), sequence_length)\n",
    "    message_third = pad_sequences(tokenizer.texts_to_sequences(texts[:, 2]), sequence_length)\n",
    "    \n",
    "    message_first = add_lexical_feature(texts, message_first, 0)\n",
    "    message_second = add_lexical_feature(texts, message_second, 1)\n",
    "    message_third = add_lexical_feature(texts, message_third, 2)\n",
    "    \n",
    "    return message_first, message_second, message_third\n",
    "\n",
    "\n",
    "message_first_message_train, message_second_message_train, message_third_message_train = get_sequances(X_train, MAX_SEQUENCE_LENGTH)\n",
    "message_first_message_val, message_second_message_val, message_third_message_val = get_sequances(X_val, MAX_SEQUENCE_LENGTH)\n",
    "message_first_message_dev, message_second_message_dev, message_third_message_dev = get_sequances(texts_dev, MAX_SEQUENCE_LENGTH)\n",
    "message_first_message_test, message_second_message_test, message_third_message_test = get_sequances(texts_test, MAX_SEQUENCE_LENGTH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Bidirectional LSTM "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Input, Dense, Embedding, Concatenate, Activation, \\\n",
    "    Dropout, LSTM, Bidirectional, GlobalMaxPooling1D, GaussianNoise\n",
    "from tensorflow.keras.models import Model\n",
    "\n",
    "\n",
    "def buildModel(embeddings_matrix, sequence_length, lstm_dim, hidden_layer_dim, num_classes, \n",
    "               noise=0.1, dropout_lstm=0.2, dropout=0.2):\n",
    "    turn1_input = Input(shape=(sequence_length,), dtype='int32')\n",
    "    turn2_input = Input(shape=(sequence_length,), dtype='int32')\n",
    "    turn3_input = Input(shape=(sequence_length,), dtype='int32')\n",
    "    embedding_dim = embeddings_matrix.shape[1]\n",
    "    embeddingLayer = Embedding(embeddings_matrix.shape[0],\n",
    "                                embedding_dim,\n",
    "                                weights=[embeddings_matrix],\n",
    "                                input_length=sequence_length,\n",
    "                                trainable=False)\n",
    "    \n",
    "    turn1_branch = embeddingLayer(turn1_input)\n",
    "    turn2_branch = embeddingLayer(turn2_input) \n",
    "    turn3_branch = embeddingLayer(turn3_input) \n",
    "    \n",
    "    turn1_branch = GaussianNoise(noise, input_shape=(None, sequence_length, embedding_dim))(turn1_branch)\n",
    "    turn2_branch = GaussianNoise(noise, input_shape=(None, sequence_length, embedding_dim))(turn2_branch)\n",
    "    turn3_branch = GaussianNoise(noise, input_shape=(None, sequence_length, embedding_dim))(turn3_branch)\n",
    "\n",
    "    lstm1 = Bidirectional(LSTM(lstm_dim, dropout=dropout_lstm))\n",
    "    lstm2 = Bidirectional(LSTM(lstm_dim, dropout=dropout_lstm))\n",
    "    \n",
    "    turn1_branch = lstm1(turn1_branch)\n",
    "    turn2_branch = lstm2(turn2_branch)\n",
    "    turn3_branch = lstm1(turn3_branch)\n",
    "    \n",
    "    x = Concatenate(axis=-1)([turn1_branch, turn2_branch, turn3_branch])\n",
    "    \n",
    "    x = Dropout(dropout)(x)\n",
    "    \n",
    "    x = Dense(hidden_layer_dim, activation='relu')(x)\n",
    "    \n",
    "    output = Dense(num_classes, activation='softmax')(x)\n",
    "    \n",
    "    model = Model(inputs=[turn1_input, turn2_input, turn3_input], outputs=output)\n",
    "    \n",
    "    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['acc'])\n",
    "    \n",
    "    return model\n",
    "\n",
    "model = buildModel(embeddings_matrix, MAX_SEQUENCE_LENGTH + 1, lstm_dim=64, hidden_layer_dim=30, num_classes=4) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"functional_5\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_7 (InputLayer)            [(None, 25)]         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_8 (InputLayer)            [(None, 25)]         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_9 (InputLayer)            [(None, 25)]         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_2 (Embedding)         (None, 25, 300)      197653800   input_7[0][0]                    \n",
      "                                                                 input_8[0][0]                    \n",
      "                                                                 input_9[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "gaussian_noise_6 (GaussianNoise (None, 25, 300)      0           embedding_2[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "gaussian_noise_7 (GaussianNoise (None, 25, 300)      0           embedding_2[1][0]                \n",
      "__________________________________________________________________________________________________\n",
      "gaussian_noise_8 (GaussianNoise (None, 25, 300)      0           embedding_2[2][0]                \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_4 (Bidirectional) (None, 128)          186880      gaussian_noise_6[0][0]           \n",
      "                                                                 gaussian_noise_8[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_5 (Bidirectional) (None, 128)          186880      gaussian_noise_7[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_2 (Concatenate)     (None, 384)          0           bidirectional_4[0][0]            \n",
      "                                                                 bidirectional_5[0][0]            \n",
      "                                                                 bidirectional_4[1][0]            \n",
      "__________________________________________________________________________________________________\n",
      "dropout_2 (Dropout)             (None, 384)          0           concatenate_2[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_4 (Dense)                 (None, 30)           11550       dropout_2[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_5 (Dense)                 (None, 4)            124         dense_4[0][0]                    \n",
      "==================================================================================================\n",
      "Total params: 198,039,234\n",
      "Trainable params: 385,434\n",
      "Non-trainable params: 197,653,800\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score, precision_score, recall_score\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, TensorBoard\n",
    "\n",
    "metrics = {\n",
    "    \"f1_selected\": (lambda y_test, y_pred:\n",
    "             f1_score(y_test, y_pred, average='micro',\n",
    "                      labels=[emotion2label['happy'],\n",
    "                              emotion2label['sad'],\n",
    "                              emotion2label['angry']\n",
    "                              ]))\n",
    "}\n",
    "\n",
    "_datasets = {}\n",
    "_datasets[\"dev\"] = [[message_first_message_dev, message_second_message_dev, message_third_message_dev],\n",
    "                    np.array(labels_categorical_dev)]\n",
    "_datasets[\"val\"] = [[message_first_message_val, message_second_message_val, message_third_message_val],\n",
    "                    np.array(labels_categorical_val)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "121/121 [==============================] - 54s 448ms/step - loss: 0.2072 - acc: 0.9247 - val_loss: 0.2530 - val_acc: 0.9082\n",
      "Epoch 2/10\n",
      "121/121 [==============================] - 54s 444ms/step - loss: 0.1958 - acc: 0.9300 - val_loss: 0.2515 - val_acc: 0.9169\n",
      "Epoch 3/10\n",
      "121/121 [==============================] - 54s 445ms/step - loss: 0.1842 - acc: 0.9336 - val_loss: 0.2507 - val_acc: 0.9120\n",
      "Epoch 4/10\n",
      "121/121 [==============================] - 54s 446ms/step - loss: 0.1773 - acc: 0.9373 - val_loss: 0.2540 - val_acc: 0.9105\n",
      "Epoch 5/10\n",
      "121/121 [==============================] - 54s 449ms/step - loss: 0.1689 - acc: 0.9401 - val_loss: 0.2530 - val_acc: 0.9093\n",
      "Epoch 6/10\n",
      "121/121 [==============================] - 56s 463ms/step - loss: 0.1574 - acc: 0.9441 - val_loss: 0.2610 - val_acc: 0.9136\n",
      "Epoch 7/10\n",
      "121/121 [==============================] - 54s 445ms/step - loss: 0.1541 - acc: 0.9473 - val_loss: 0.2610 - val_acc: 0.9125\n",
      "Epoch 8/10\n",
      "121/121 [==============================] - 54s 445ms/step - loss: 0.1485 - acc: 0.9480 - val_loss: 0.2723 - val_acc: 0.9120\n",
      "Epoch 9/10\n",
      "121/121 [==============================] - 54s 444ms/step - loss: 0.1364 - acc: 0.9515 - val_loss: 0.2670 - val_acc: 0.9085\n",
      "Epoch 10/10\n",
      "121/121 [==============================] - 54s 447ms/step - loss: 0.1288 - acc: 0.9545 - val_loss: 0.2844 - val_acc: 0.9090\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x21024d13898>"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit([message_first_message_train, message_second_message_train, message_third_message_train],\n",
    "                    np.array(labels_categorical_train),\n",
    "                    validation_data=(\n",
    "                        [message_first_message_val, message_second_message_val, message_third_message_val],\n",
    "                        np.array(labels_categorical_val)\n",
    "                    ),\n",
    "                    epochs=10,\n",
    "                    batch_size=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = model.predict([message_first_message_dev, message_second_message_dev, message_third_message_dev])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1_selected 0.7023686920700309\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      0.91      0.94      2338\n",
      "           1       0.66      0.75      0.70       142\n",
      "           2       0.66      0.84      0.74       125\n",
      "           3       0.56      0.86      0.68       150\n",
      "\n",
      "    accuracy                           0.90      2755\n",
      "   macro avg       0.71      0.84      0.76      2755\n",
      "weighted avg       0.92      0.90      0.91      2755\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "for title, metric in metrics.items():\n",
    "    print(title, metric(labels_categorical_dev.argmax(axis=1), y_pred.argmax(axis=1)))\n",
    "print(classification_report(labels_categorical_dev.argmax(axis=1), y_pred.argmax(axis=1)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Performance Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1_selected 0.701500258665287\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      0.91      0.94      4677\n",
      "           1       0.70      0.71      0.71       284\n",
      "           2       0.61      0.86      0.71       250\n",
      "           3       0.57      0.88      0.69       298\n",
      "\n",
      "    accuracy                           0.90      5509\n",
      "   macro avg       0.71      0.84      0.76      5509\n",
      "weighted avg       0.92      0.90      0.91      5509\n",
      "\n"
     ]
    }
   ],
   "source": [
    "y_pred = model.predict([message_first_message_test, message_second_message_test, message_third_message_test])\n",
    "\n",
    "for title, metric in metrics.items():\n",
    "    print(title, metric(labels_categorical_test.argmax(axis=1), y_pred.argmax(axis=1)))\n",
    "print(classification_report(labels_categorical_test.argmax(axis=1), y_pred.argmax(axis=1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1_selected 0.719832109129066\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      0.92      0.95      4677\n",
      "           1       0.60      0.80      0.68       284\n",
      "           2       0.70      0.84      0.76       250\n",
      "           3       0.63      0.84      0.72       298\n",
      "\n",
      "    accuracy                           0.91      5509\n",
      "   macro avg       0.73      0.85      0.78      5509\n",
      "weighted avg       0.92      0.91      0.91      5509\n",
      "\n"
     ]
    }
   ],
   "source": [
    "y_pred = model.predict([message_first_message_test, message_second_message_test, message_third_message_test])\n",
    "\n",
    "for title, metric in metrics.items():\n",
    "    print(title, metric(labels_categorical_test.argmax(axis=1), y_pred.argmax(axis=1)))\n",
    "print(classification_report(labels_categorical_test.argmax(axis=1), y_pred.argmax(axis=1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def saveSubmissionFile(solution_path, test_data_path, predictions):\n",
    "    with io.open(solution_path, \"w\", encoding=\"utf8\") as fout:\n",
    "        fout.write('\\t'.join([\"id\", \"turn1\", \"turn2\", \"turn3\", \"label\"]) + '\\n')        \n",
    "        with io.open(test_data_path, encoding=\"utf8\") as fin:\n",
    "            fin.readline()\n",
    "            for lineNum, line in enumerate(fin):\n",
    "                fout.write('\\t'.join(line.strip().split('\\t')[:4]) + '\\t')\n",
    "                fout.write(label2emotion[predictions[lineNum]] + '\\n')\n",
    "                \n",
    "                \n",
    "saveSubmissionFile('results.txt', './starterkitdata/test.txt', y_pred.argmax(axis=1))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
